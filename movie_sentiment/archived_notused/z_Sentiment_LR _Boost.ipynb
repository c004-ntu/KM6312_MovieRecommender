{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Sentiment Analysis Of Movie Reviews </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10468, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "rating_df = pd.read_csv('data/rating_auto_label_sentiment_two_classes.csv')\n",
    "\n",
    "# drop unused columns\n",
    "rating_df = rating_df [['review_text','sentiment']]\n",
    "rating_df.head(2)\n",
    "rating_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10462, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with NaN values in any column\n",
    "rating_df = rating_df.dropna()\n",
    "rating_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Text Pre-processing\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): #ADJECTIVE\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): #VERN\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): #NOUN        \n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): #ADVERB\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # If no tag was found, then use the word as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # Else use the tag to lemmatize the word\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df['review_text'] = df['review_text'].astype(str).fillna('')\n",
    "\n",
    "    # remove white space\n",
    "    df['review_text'] = df['review_text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # update to lower case\n",
    "    df['review_text'] = df['review_text'].str.lower()\n",
    "\n",
    "    # remove punctuations\n",
    "    df['review_text'] = df['review_text'].str.replace(r'[{}]'.format(re.escape(string.punctuation)), '', regex=True)\n",
    "\n",
    "    # remove special characters\n",
    "    df['review_text'] = df['review_text'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "\n",
    "    # remove digits\n",
    "    df['review_text'] = df['review_text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "    # remove non ascii\n",
    "    df['review_text'] = df['review_text'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "    \n",
    "    # remove URL\n",
    "    df['review_text'] = df['review_text'].str.replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    stop_words = stopwords.words('english') + ['br']\n",
    "    stopwords_dict = Counter(stop_words)\n",
    "    df['review_text'] = df['review_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_dict]))\n",
    "\n",
    "    return df\n",
    "\n",
    "def lemmatize(df):\n",
    "    df['review_text'] = df['review_text'].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_result = pd.DataFrame(columns=['model', 'task_no', 'vectorizer', 'ngram', 'max_iter', 'C', 'gamma', 'n_estimator', 'lrate', 'test_accuracy', 'wall_time','run_time'])\n",
    "model_no = 1\n",
    "filename=\"output/result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>let leave door love beetlejuice edward scissor...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fast paced action thriller delivers begin end ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>excellent movie great cast see movie saw one r...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>write three highpraise review try think bad mo...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well make movie quality write act cinematograp...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text sentiment\n",
       "0  let leave door love beetlejuice edward scissor...  NEGATIVE\n",
       "1  fast paced action thriller delivers begin end ...  POSITIVE\n",
       "2  excellent movie great cast see movie saw one r...  POSITIVE\n",
       "3  write three highpraise review try think bad mo...  NEGATIVE\n",
       "4  well make movie quality write act cinematograp...  POSITIVE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Preprocessing\n",
    "rating_df = preprocess_text(rating_df)\n",
    "rating_df = remove_stopwords(rating_df)\n",
    "rating_df = lemmatize(rating_df)\n",
    "\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8369,)\n",
      "(1046,)\n",
      "(1047,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(rating_df.review_text,rating_df.sentiment,test_size = 0.2, random_state=42)\n",
    "\n",
    "# 80% training, 20% temporary\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(rating_df.review_text, rating_df.sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "# 10% validation, 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.97 s\n",
      "Wall time: 3.42 s\n",
      "AdaBoost_LogReg - 1, text_preprocess: True, vectorizer: tfidf, ngram: 2, max_iter: 100000, n_estimate:100, lrate:1.0\n",
      "Test Accuracy: 0.7440305635148042\n",
      "\n",
      "CPU times: total: 39 s\n",
      "Wall time: 1min 9s\n",
      "AdaBoost_LogReg - 2, text_preprocess: True, vectorizer: tfidf, ngram: 2, max_iter: 100000, n_estimate:100, lrate:0.001\n",
      "Test Accuracy: 0.7554918815663801\n",
      "\n",
      "CPU times: total: 38.4 s\n",
      "Wall time: 58.6 s\n",
      "AdaBoost_LogReg - 3, text_preprocess: True, vectorizer: tfidf, ngram: 2, max_iter: 100000, n_estimate:100, lrate:0.0001\n",
      "Test Accuracy: 0.7535816618911175\n",
      "\n",
      "CPU times: total: 1.98 s\n",
      "Wall time: 2.78 s\n",
      "AdaBoost_LogReg - 4, text_preprocess: True, vectorizer: tfidf, ngram: 2, max_iter: 100000, n_estimate:1000, lrate:1.0\n",
      "Test Accuracy: 0.7440305635148042\n",
      "\n",
      "CPU times: total: 4min 57s\n",
      "Wall time: 7min 49s\n",
      "AdaBoost_LogReg - 5, text_preprocess: True, vectorizer: tfidf, ngram: 2, max_iter: 100000, n_estimate:1000, lrate:0.001\n",
      "Test Accuracy: 0.7593123209169055\n",
      "\n",
      "CPU times: total: 49.6 s\n",
      "Wall time: 2min 55s\n",
      "AdaBoost_LogReg - 6, text_preprocess: True, vectorizer: tfidf, ngram: 2, max_iter: 100000, n_estimate:1000, lrate:0.0001\n",
      "Test Accuracy: 0.7583572110792741\n",
      "\n",
      "CPU times: total: 453 ms\n",
      "Wall time: 1.85 s\n",
      "AdaBoost_LogReg - 7, text_preprocess: True, vectorizer: tfidf, ngram: 3, max_iter: 100000, n_estimate:100, lrate:1.0\n",
      "Test Accuracy: 0.7526265520534862\n",
      "\n",
      "CPU times: total: 17.2 s\n",
      "Wall time: 47.7 s\n",
      "AdaBoost_LogReg - 8, text_preprocess: True, vectorizer: tfidf, ngram: 3, max_iter: 100000, n_estimate:100, lrate:0.001\n",
      "Test Accuracy: 0.7507163323782235\n",
      "\n",
      "CPU times: total: 32.9 s\n",
      "Wall time: 1min 11s\n",
      "AdaBoost_LogReg - 9, text_preprocess: True, vectorizer: tfidf, ngram: 3, max_iter: 100000, n_estimate:100, lrate:0.0001\n",
      "Test Accuracy: 0.7535816618911175\n",
      "\n",
      "CPU times: total: 1.03 s\n",
      "Wall time: 2.56 s\n",
      "AdaBoost_LogReg - 10, text_preprocess: True, vectorizer: tfidf, ngram: 3, max_iter: 100000, n_estimate:1000, lrate:1.0\n",
      "Test Accuracy: 0.7526265520534862\n",
      "\n",
      "CPU times: total: 2min 49s\n",
      "Wall time: 8min 15s\n",
      "AdaBoost_LogReg - 11, text_preprocess: True, vectorizer: tfidf, ngram: 3, max_iter: 100000, n_estimate:1000, lrate:0.001\n",
      "Test Accuracy: 0.7488061127029608\n",
      "\n",
      "CPU times: total: 2min\n",
      "Wall time: 7min 3s\n",
      "AdaBoost_LogReg - 12, text_preprocess: True, vectorizer: tfidf, ngram: 3, max_iter: 100000, n_estimate:1000, lrate:0.0001\n",
      "Test Accuracy: 0.7545367717287488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression models\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "n_vects = ['tfidf']\n",
    "n_grams = [2,3]\n",
    "n_iters = [100000]\n",
    "n_estimators=[100, 1000]\n",
    "lrates = [1.0, 0.001, 0.0001]\n",
    "# n_stop_lemmatize = [False, True]\n",
    "\n",
    "for n_gram in n_grams:\n",
    "    for n_vect in n_vects:\n",
    "        if n_vect=='cbow':\n",
    "            # Use all features, remove stopwords, apply unigram, bigram, trigram\n",
    "            vect = CountVectorizer(max_features=None, ngram_range=(1,n_gram), stop_words='english', lowercase=True, strip_accents='ascii')\n",
    "        else:\n",
    "            vect = TfidfVectorizer(max_features=None, ngram_range=(1,n_gram), stop_words='english', lowercase=True, strip_accents='ascii')\n",
    "\n",
    "        # Fit on training data and transform the training data to vector (document-term matrix)\n",
    "        X_train_dtm = vect.fit_transform(X_train)\n",
    "        # display(X_train_dtm)\n",
    "\n",
    "        X_val_dtm = vect.transform(X_val)\n",
    "        # display(X_val_dtm)\n",
    "\n",
    "        X_test_dtm = vect.transform(X_test)\n",
    "        # display(X_test_dtm)\n",
    "\n",
    "        for n_iter in n_iters:\n",
    "            for n_estimate in n_estimators:\n",
    "                for lrate in lrates:\n",
    "                    # Initialize the LogisticRegression classifier\n",
    "                    logreg =  LogisticRegression(max_iter=n_iter, class_weight='balanced', random_state=42)\n",
    "\n",
    "                    # Initialize the AdaBoostClassifier with LogisticRegression as the base estimator\n",
    "                    ensemble_model = AdaBoostClassifier(estimator=logreg, n_estimators=n_estimate, learning_rate=lrate, random_state=42, algorithm='SAMME')\n",
    "\n",
    "                    # Train the classifier on the training data & capture wall time\n",
    "                    start_time = time.time()\n",
    "                    %time ensemble_model.fit(X_train_dtm, y_train)\n",
    "                    end_time = time.time()\n",
    "                    wall_time = end_time - start_time\n",
    "\n",
    "                    # Predict and evaluate the classifier\n",
    "                    y_val_pred = ensemble_model.predict(X_val_dtm)\n",
    "                    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "                    # Final evaluation on test set after tuning\n",
    "                    y_test_pred = ensemble_model.predict(X_test_dtm)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "                    # Print result\n",
    "                    task_no = str(model_no)\n",
    "                    model = 'AdaBoost_LogReg'\n",
    "                    print(f\"{model} - {task_no}, text_preprocess: {True}, vectorizer: {n_vect}, ngram: {n_gram}, max_iter: {n_iter}, n_estimate:{n_estimate}, lrate:{lrate}\")\n",
    "                    print(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "                    model_no +=1\n",
    "\n",
    "                    # Record result to dataframe, to be exported to csv\n",
    "                    new_row = [model, task_no, n_vect, n_gram, n_iter, '', '', n_estimate, lrate, test_accuracy, wall_time, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")]\n",
    "                    df_result.loc[len(df_result)] = new_row\n",
    "\n",
    "                    new_row_df = pd.DataFrame([new_row], columns=df_result.columns)\n",
    "                    new_row_df.to_csv(filename, index=False, mode='a', header=not os.path.exists(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.68      0.64      0.66       388\n",
      "    POSITIVE       0.79      0.82      0.81       659\n",
      "\n",
      "    accuracy                           0.75      1047\n",
      "   macro avg       0.74      0.73      0.73      1047\n",
      "weighted avg       0.75      0.75      0.75      1047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "Report=classification_report(y_test,y_test_pred)\n",
    "print(Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Predictions: ['POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "# Inference on new data\n",
    "# new_reviews = ['A worthy contender for the Animated film of 2024', 'No plot at all. But if you are looking for a good laugh. You will not find that either.']\n",
    "new_reviews = [\n",
    "    \"I absolutely love this movie! It was amazing.\",\n",
    "    \"This movie was terrible, I hated every second of it.\", \n",
    "    \"while this movie is not intended for everyone, it is good for someone has no brain\", \n",
    "    \"let's watch it only when it is free to watch, i will not pay for it\",\n",
    "    'A worthy contender for the Animated film of 2024', \n",
    "    'No plot at all. But if you are looking for a good laugh. You will not find that either.'\n",
    "]\n",
    "\n",
    "new_reviews_dtm = vect.transform(new_reviews)\n",
    "new_predictions = ensemble_model.predict(new_reviews_dtm)\n",
    "\n",
    "print(\"New Predictions:\", new_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
