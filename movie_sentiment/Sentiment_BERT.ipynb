{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28773591-e310-4a82-b549-13374cd77d40",
   "metadata": {
    "id": "28773591-e310-4a82-b549-13374cd77d40",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ceffendy\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, get_scheduler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a00bc-2c81-4792-881e-36f5717aa496",
   "metadata": {
    "id": "1a0a00bc-2c81-4792-881e-36f5717aa496",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54c95c7-62af-4fac-a453-3a3883d86397",
   "metadata": {
    "id": "d54c95c7-62af-4fac-a453-3a3883d86397",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10468, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "root_path = \".\"\n",
    "df = pd.read_csv(f'{root_path}/data/rating_auto_label_sentiment_two_classes.csv')\n",
    "\n",
    "# drop unused columns\n",
    "df = df [['review_text','sentiment']]\n",
    "df.head(2)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6350e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10462, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with NaN values in any column\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec6882-34d1-432e-a45e-8f1088fb84a3",
   "metadata": {
    "id": "3aec6882-34d1-432e-a45e-8f1088fb84a3",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1: Split the Data into Train, Test, and Eval Sets\n",
    "We can use sklearn to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27bf0f9-d172-4fe0-b8a0-c89be9adc302",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f27bf0f9-d172-4fe0-b8a0-c89be9adc302",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "7664855f-7c95-4ba9-ba36-bc8572b65561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8369, Test size: 1046, Eval size: 1047\n"
     ]
    }
   ],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, temp_idx in splitter.split(df, df['sentiment']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    temp_df = df.iloc[temp_idx]\n",
    "\n",
    "# Split temp_df into test and eval (50% each)\n",
    "test_df, eval_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['sentiment'], random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}, Eval size: {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b6903-d867-4854-a664-7ba5955c5319",
   "metadata": {
    "id": "ac5b6903-d867-4854-a664-7ba5955c5319",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### init tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7f87ad-a39c-4071-8bd5-1f05439d1500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d7f87ad-a39c-4071-8bd5-1f05439d1500",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "ec5dcefb-dbb2-4d3b-8219-a40cc19c62d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ceffendy\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e05b32-634a-4a18-b0b2-f430372d9b6b",
   "metadata": {
    "id": "15e05b32-634a-4a18-b0b2-f430372d9b6b",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Data Preparation (Tokenization)\n",
    "We need to tokenize the text data and convert it into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e34b0-111f-452a-8d5e-b695fe5b759f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "0a0e34b0-111f-452a-8d5e-b695fe5b759f",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "221d32fd-c24c-47e2-9dd0-1d70d9dc61b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7927fede9b5d450e980a3ba59385654e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2204, 'grad_norm': 8.525217056274414, 'learning_rate': 1.3638676844783715e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9029ea1f1fc84530bdc308c4a69d141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19172503054141998, 'eval_accuracy': 0.9312320916905444, 'eval_f1': 0.9451219512195121, 'eval_precision': 0.9351432880844646, 'eval_recall': 0.9553158705701078, 'eval_runtime': 165.314, 'eval_samples_per_second': 6.333, 'eval_steps_per_second': 0.399, 'epoch': 1.0}\n",
      "{'loss': 0.1078, 'grad_norm': 54.804473876953125, 'learning_rate': 7.27735368956743e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c4f3bfdefb40269a75cf1d86b78e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34871822595596313, 'eval_accuracy': 0.9340974212034384, 'eval_f1': 0.9478458049886621, 'eval_precision': 0.93026706231454, 'eval_recall': 0.9661016949152542, 'eval_runtime': 138.1068, 'eval_samples_per_second': 7.581, 'eval_steps_per_second': 0.478, 'epoch': 2.0}\n",
      "{'loss': 0.0516, 'grad_norm': 0.1546747237443924, 'learning_rate': 9.160305343511451e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d1b10b232540fbb71d0fab3f398be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3410106301307678, 'eval_accuracy': 0.9379178605539638, 'eval_f1': 0.9503437738731857, 'eval_precision': 0.9424242424242424, 'eval_recall': 0.9583975346687211, 'eval_runtime': 76.8551, 'eval_samples_per_second': 13.623, 'eval_steps_per_second': 0.859, 'epoch': 3.0}\n",
      "{'train_runtime': 14348.7688, 'train_samples_per_second': 1.75, 'train_steps_per_second': 0.11, 'train_loss': 0.12268722739838461, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0045676b2c27469faf470fea5b863937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.19172503054141998, 'eval_accuracy': 0.9312320916905444, 'eval_f1': 0.9451219512195121, 'eval_precision': 0.9351432880844646, 'eval_recall': 0.9553158705701078, 'eval_runtime': 86.1908, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.766, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ac858feb7b476c910d4db28c242a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.17893357574939728, 'eval_accuracy': 0.9388145315487572, 'eval_f1': 0.9511450381679389, 'eval_precision': 0.9425113464447806, 'eval_recall': 0.9599383667180277, 'eval_runtime': 100.0609, 'eval_samples_per_second': 10.454, 'eval_steps_per_second': 0.66, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Define the SentimentDataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        label = 1 if row['sentiment'] == 'POSITIVE' else 0\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_df, tokenizer)\n",
    "test_dataset = SentimentDataset(test_df, tokenizer)\n",
    "eval_dataset = SentimentDataset(eval_df, tokenizer)\n",
    "\n",
    "# Handle Class Imbalance with Weighted Loss\n",
    "class_counts = df['sentiment'].value_counts().to_dict()\n",
    "total_samples = len(df)\n",
    "weights = {\n",
    "    0: total_samples / class_counts['NEGATIVE'],\n",
    "    1: total_samples / class_counts['POSITIVE']\n",
    "}\n",
    "class_weights = torch.tensor([weights[0], weights[1]], dtype=torch.float)\n",
    "\n",
    "# Initialize Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    num_labels=2\n",
    ")\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu=True,  # Force training on CPU\n",
    "    disable_tqdm=False,  # Enable progress bar\n",
    "    report_to=\"none\",  # Disable wandb logging\n",
    ")\n",
    "\n",
    "# Define Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Custom Trainer to handle class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss with class weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Initialize Trainer with Early Stopping Callback\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the Best Model on the Evaluation set\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Evaluate the Best Model on the Test Set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd54d1-fa90-4993-b301-46840d390bbb",
   "metadata": {
    "id": "a5cd54d1-fa90-4993-b301-46840d390bbb",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c4e6c-9249-4ffd-b38b-b4d8f08e94b7",
   "metadata": {
    "id": "141c4e6c-9249-4ffd-b38b-b4d8f08e94b7",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been zipped and saved to output foder successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import IPython.display as display\n",
    "model_name = \"distilbert-sst-2\"\n",
    "output_folder = f\"./output/{model_name}\"\n",
    "model.save_pretrained(output_folder)\n",
    "tokenizer.save_pretrained(output_folder)\n",
    "\n",
    "# Create a zip file from the final_model folder and save it to the output folder\n",
    "with zipfile.ZipFile(f\"./output/{model_name}.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, output_folder))\n",
    "\n",
    "print(\"Model has been zipped and saved to output foder successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33009df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been zipped and saved to output foder successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import IPython.display as display\n",
    "model_name = \"fine_tuned_model\"\n",
    "output_folder = f\"./output/{model_name}\"\n",
    "model.save_pretrained(output_folder)\n",
    "tokenizer.save_pretrained(output_folder)\n",
    "\n",
    "# Create a zip file from the final_model folder and save it to disk\n",
    "with zipfile.ZipFile(f\"./output/{model_name}.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, output_folder))\n",
    "\n",
    "print(\"Model has been zipped and saved to output folder successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test results\n",
    "df_result = pd.DataFrame(columns=['model', 'task_no', 'vectorizer', 'ngram', 'max_iter', 'C', 'gamma', 'tree_param', 'n_estimator', 'lrate', 'test_accuracy', 'wall_time','run_time'])\n",
    "model_no = 1\n",
    "filename=\"output/result_DB.csv\"\n",
    "\n",
    "# Print result\n",
    "task_no = str(model_no)\n",
    "model = 'Distill'\n",
    "print(f\"{model} - {task_no}, text_preprocess: {True}, vectorizer: {'WordPiece'}, lrate: {'-'}, batch:{'-'}, num_epoch:{'-'}, weight_decay:{-})\n",
    "print(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "model_no +=1\n",
    "\n",
    "# Record result to dataframe, to be exported to csv\n",
    "# columns=['model', 'task_no', 'vectorizer', 'ngram', 'max_iter', 'C', 'gamma', 'tree_param', 'n_estimator', 'lrate', 'test_accuracy', 'wall_time','run_time']\n",
    "new_row = [model, task_no, n_vect, n_gram, n_iter, C, gamma, '', 0, 0, test_accuracy, wall_time, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")]\n",
    "df_result.loc[len(df_result)] = new_row\n",
    "\n",
    "new_row_df = pd.DataFrame([new_row], columns=df_result.columns)\n",
    "new_row_df.to_csv(filename, index=False, mode='a', header=not os.path.exists(filename))\n",
    "\n",
    "# Check for the best model\n",
    "if test_accuracy > best_accuracy:\n",
    "    best_model = f\"{model} - {task_no}, text_preprocess: {True}, vectorizer: {n_vect}, ngram: {n_gram}, max_iter: {n_iter}, C:{C}, gamma:{gamma}\"\n",
    "    best_accuracy = test_accuracy\n",
    "    best_y_test_pred = y_test_pred\n",
    "\n",
    "for key, value in test_results.items():\n",
    "    if key=='eval_accuracy':\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075ef1b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a870f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "import torch.nn.functional as F  # To use softmax\n",
    "\n",
    "# Path to the saved model and tokenizer\n",
    "model_path = './output/distilbert-sst-2'\n",
    "\n",
    "# Load the model and tokenizer from the saved path\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode (disable dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "def preprocess_text(text, tokenizer, max_length=128):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    # Preprocess the input text\n",
    "    encoding = preprocess_text(text, tokenizer)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits  # Get the raw prediction scores\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1)  # Get the predicted class (0 for negative, 1 for positive)\n",
    "    \n",
    "    # Map the predicted class to sentiment label\n",
    "    sentiment = 'POSITIVE' if predicted_class.item() == 1 else 'NEGATIVE'\n",
    "    \n",
    "    # Return sentiment and probabilities for each class\n",
    "    positive_prob = probabilities[0][1].item()  # Probability for POSITIVE class\n",
    "    negative_prob = probabilities[0][0].item()  # Probability for NEGATIVE class\n",
    "    \n",
    "    return sentiment, positive_prob, negative_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b852a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I absolutely love this movie! It was amazing.\n",
      "Predicted Sentiment: POSITIVE\n",
      "POSITIVE: 0.9998, NEGATIVE: 0.0002\n",
      "\n",
      "Sentence: This movie was terrible, I hated every second of it.\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0004, NEGATIVE: 0.9996\n",
      "\n",
      "Sentence: while this movie is not intended for everyone, it is good for someone has no brain\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0683, NEGATIVE: 0.9317\n",
      "\n",
      "Sentence: let's watch it only when it is free to watch, i will not pay for it\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0671, NEGATIVE: 0.9329\n",
      "\n",
      "Sentence: A worthy contender for the Animated film of 2024\n",
      "Predicted Sentiment: POSITIVE\n",
      "POSITIVE: 0.9996, NEGATIVE: 0.0004\n",
      "\n",
      "Sentence: No plot at all. But if you are looking for a good laugh. You will not find that either.\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0017, NEGATIVE: 0.9983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences (positive and negative examples)\n",
    "sample_sentences = [\n",
    "    \"I absolutely love this movie! It was amazing.\",\n",
    "    \"This movie was terrible, I hated every second of it.\", \n",
    "    \"while this movie is not intended for everyone, it is good for someone has no brain\", \n",
    "    \"let's watch it only when it is free to watch, i will not pay for it\",\n",
    "    'A worthy contender for the Animated film of 2024', \n",
    "    'No plot at all. But if you are looking for a good laugh. You will not find that either.'\n",
    "]\n",
    "\n",
    "# Perform inference on each sample sentence\n",
    "for sentence in sample_sentences:\n",
    "    sentiment, positive_prob, negative_prob = predict_sentiment(sentence, model, tokenizer)\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {sentiment}\")\n",
    "    print(f\"POSITIVE: {positive_prob:.4f}, NEGATIVE: {negative_prob:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "dependencies": {
   "environment": {
    "environmentId": "1ae0f57a-c623-40c0-af33-c7d2e8050b04",
    "workspaceId": "b50088d4-10d0-4364-aac0-6ff94657d950"
   },
   "lakehouse": {
    "default_lakehouse": "ec1c56f5-c377-41bf-9ed7-e34e90cf6380",
    "default_lakehouse_name": "Christine",
    "default_lakehouse_workspace_id": "b50088d4-10d0-4364-aac0-6ff94657d950"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
