{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0944c9",
   "metadata": {},
   "source": [
    "## <center> Sentiment Analysis Of Movie Reviews (distilbert-base-uncased-finetuned-sst-2-english) </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28773591-e310-4a82-b549-13374cd77d40",
   "metadata": {
    "id": "28773591-e310-4a82-b549-13374cd77d40",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ceffendy\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, get_scheduler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a00bc-2c81-4792-881e-36f5717aa496",
   "metadata": {
    "id": "1a0a00bc-2c81-4792-881e-36f5717aa496",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54c95c7-62af-4fac-a453-3a3883d86397",
   "metadata": {
    "id": "d54c95c7-62af-4fac-a453-3a3883d86397",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10468, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "root_path = \".\"\n",
    "df = pd.read_csv(f'{root_path}/data/rating_auto_label_sentiment_two_classes.csv')\n",
    "\n",
    "# drop unused columns\n",
    "df = df [['review_text','sentiment']]\n",
    "df.head(2)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6350e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10462, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with NaN values in any column\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec6882-34d1-432e-a45e-8f1088fb84a3",
   "metadata": {
    "id": "3aec6882-34d1-432e-a45e-8f1088fb84a3",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Split the Data into Train, Test, and Eval Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27bf0f9-d172-4fe0-b8a0-c89be9adc302",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f27bf0f9-d172-4fe0-b8a0-c89be9adc302",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "7664855f-7c95-4ba9-ba36-bc8572b65561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8369, Test size: 1046, Eval size: 1047\n"
     ]
    }
   ],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, temp_idx in splitter.split(df, df['sentiment']):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    temp_df = df.iloc[temp_idx]\n",
    "\n",
    "# Split temp_df into test and eval (50% each)\n",
    "test_df, eval_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['sentiment'], random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}, Eval size: {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b6903-d867-4854-a664-7ba5955c5319",
   "metadata": {
    "id": "ac5b6903-d867-4854-a664-7ba5955c5319",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7f87ad-a39c-4071-8bd5-1f05439d1500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d7f87ad-a39c-4071-8bd5-1f05439d1500",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "ec5dcefb-dbb2-4d3b-8219-a40cc19c62d7"
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e05b32-634a-4a18-b0b2-f430372d9b6b",
   "metadata": {
    "id": "15e05b32-634a-4a18-b0b2-f430372d9b6b",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Data Preparation (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0e34b0-111f-452a-8d5e-b695fe5b759f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "0a0e34b0-111f-452a-8d5e-b695fe5b759f",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "221d32fd-c24c-47e2-9dd0-1d70d9dc61b0"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Define the SentimentDataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        label = 1 if row['sentiment'] == 'POSITIVE' else 0\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Custom Trainer to handle class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss with class weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_df, tokenizer)\n",
    "test_dataset = SentimentDataset(test_df, tokenizer)\n",
    "eval_dataset = SentimentDataset(eval_df, tokenizer)\n",
    "\n",
    "# Handle Class Imbalance with Weighted Loss\n",
    "class_counts = df['sentiment'].value_counts().to_dict()\n",
    "total_samples = len(df)\n",
    "weights = {\n",
    "    0: total_samples / class_counts['NEGATIVE'],\n",
    "    1: total_samples / class_counts['POSITIVE']\n",
    "}\n",
    "\n",
    "class_weights = torch.tensor([weights[0], weights[1]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aaa554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ceffendy\\AppData\\Local\\Temp\\ipykernel_11560\\2969489129.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305709ae42a5413485302fdffc53d336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2336, 'grad_norm': 23.36346435546875, 'learning_rate': 1.3638676844783715e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba9d3254bbb406b9720417008d0f693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.255489706993103, 'eval_accuracy': 0.9159503342884432, 'eval_f1': 0.9346210995542348, 'eval_precision': 0.9024390243902439, 'eval_recall': 0.9691833590138675, 'eval_runtime': 307.9387, 'eval_samples_per_second': 3.4, 'eval_steps_per_second': 0.214, 'epoch': 1.0}\n",
      "{'loss': 0.1258, 'grad_norm': 11.411917686462402, 'learning_rate': 7.27735368956743e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bbd77e0f704a9b81ca059275b824b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41435420513153076, 'eval_accuracy': 0.9121298949379179, 'eval_f1': 0.9315476190476191, 'eval_precision': 0.9007194244604316, 'eval_recall': 0.9645608628659477, 'eval_runtime': 278.3089, 'eval_samples_per_second': 3.762, 'eval_steps_per_second': 0.237, 'epoch': 2.0}\n",
      "{'loss': 0.0538, 'grad_norm': 5.007336139678955, 'learning_rate': 9.160305343511451e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f5896586ff40fb943722a2265c566e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41497743129730225, 'eval_accuracy': 0.9216809933142311, 'eval_f1': 0.9374045801526718, 'eval_precision': 0.9288956127080181, 'eval_recall': 0.9460708782742681, 'eval_runtime': 294.6742, 'eval_samples_per_second': 3.553, 'eval_steps_per_second': 0.224, 'epoch': 3.0}\n",
      "{'train_runtime': 19400.7361, 'train_samples_per_second': 1.294, 'train_steps_per_second': 0.081, 'train_loss': 0.13358036009713287, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631d7fda911b4f71bb0258a4ed648f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.255489706993103, 'eval_accuracy': 0.9159503342884432, 'eval_f1': 0.9346210995542348, 'eval_precision': 0.9024390243902439, 'eval_recall': 0.9691833590138675, 'eval_runtime': 291.8124, 'eval_samples_per_second': 3.588, 'eval_steps_per_second': 0.226, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a68f395f71497b9afa8963d308348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.23828238248825073, 'eval_accuracy': 0.9196940726577438, 'eval_f1': 0.9379615952732644, 'eval_precision': 0.900709219858156, 'eval_recall': 0.9784283513097073, 'eval_runtime': 306.824, 'eval_samples_per_second': 3.409, 'eval_steps_per_second': 0.215, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu=True,  # Force training on CPU\n",
    "    disable_tqdm=False,  # Enable progress bar\n",
    "    report_to=\"none\",  # Disable wandb logging\n",
    ")\n",
    "# Initialize Trainer with Early Stopping Callback\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "best_model = ''\n",
    "best_accuracy = 0\n",
    "best_y_test_pred = None\n",
    "\n",
    "# Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the Best Model on the Evaluation set\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Evaluate the Best Model on the Test Set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005e98bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert - 1, text_preprocess: True, vectorizer: WordPiece, lrate: 2e-05, batch:16, num_epoch:16, weight_decay:0.01\n",
      "Test Accuracy: 0.9196940726577438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Save Test results\n",
    "df_result = pd.DataFrame(columns=['model', 'task_no', 'vectorizer', 'ngram', 'max_iter', 'C', 'gamma', 'tree_param', 'n_estimator', 'lrate', 'batch_size', 'num_epoch', 'weight_decay', 'test_accuracy', 'wall_time', 'run_time'])\n",
    "model_no = 1\n",
    "filename=\"output/result_DB.csv\"\n",
    "\n",
    "# Print result\n",
    "task_no = str(model_no)\n",
    "model_algo = 'distilbert'\n",
    "print(f\"{model_algo} - {task_no}, text_preprocess: {True}, vectorizer: {'WordPiece'}, lrate: {training_args.learning_rate}, batch:{training_args.per_device_train_batch_size}, num_epoch:{training_args.per_device_train_batch_size}, weight_decay:{training_args.weight_decay}\")\n",
    "\n",
    "wall_time=0\n",
    "test_accuracy=0\n",
    "\n",
    "for key, value in test_results.items():\n",
    "    if key=='eval_accuracy':\n",
    "        test_accuracy = value\n",
    "        print(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "    if key == 'train_runtime' or key == 'eval_runtime':\n",
    "        wall_time = wall_time + float(value)\n",
    "model_no +=1\n",
    "\n",
    "# Record result to dataframe, to be exported to csv\n",
    "columns=['model', 'task_no', 'vectorizer', 'ngram', 'max_iter', 'C', 'gamma', 'tree_param', 'n_estimator', 'lrate', 'batch_size', 'num_epoch', 'weight_decay', 'test_accuracy', 'wall_time', 'run_time']\n",
    "new_row = [model_algo, task_no, 'wordpiece', 0, 0, 0, '', '', 0, training_args.learning_rate, training_args.per_device_train_batch_size, training_args.num_train_epochs, training_args.weight_decay, test_accuracy, wall_time, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")]\n",
    "df_result.loc[len(df_result)] = new_row\n",
    "\n",
    "new_row_df = pd.DataFrame([new_row], columns=df_result.columns)\n",
    "new_row_df.to_csv(filename, index=False, mode='a', header=not os.path.exists(filename))\n",
    "\n",
    "# Check for the best model\n",
    "if test_accuracy > best_accuracy:\n",
    "    best_model = f\"{model_algo} - {task_no}, text_preprocess: {True}, vectorizer: {'WordPiece'}, lrate: {training_args.learning_rate}, batch:{training_args.per_device_train_batch_size}, num_epoch:{training_args.num_train_epochs}, weight_decay:{training_args.weight_decay}\"\n",
    "    best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd54d1-fa90-4993-b301-46840d390bbb",
   "metadata": {
    "id": "a5cd54d1-fa90-4993-b301-46840d390bbb",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "141c4e6c-9249-4ffd-b38b-b4d8f08e94b7",
   "metadata": {
    "id": "141c4e6c-9249-4ffd-b38b-b4d8f08e94b7",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been zipped and saved to output foder successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import IPython.display as display\n",
    "model_name = \"distilbert\"\n",
    "output_folder = f\"./output/{model_name}\"\n",
    "model.save_pretrained(output_folder)\n",
    "tokenizer.save_pretrained(output_folder)\n",
    "\n",
    "# Create a zip file from the final_model folder and save it to the output folder\n",
    "with zipfile.ZipFile(f\"./output/{model_name}.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, output_folder))\n",
    "\n",
    "print(\"Model has been zipped and saved to output foder successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075ef1b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a870f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "import torch.nn.functional as F  # To use softmax\n",
    "\n",
    "# Path to the saved model and tokenizer\n",
    "model_path = './output/distilbert'\n",
    "\n",
    "# Load the model and tokenizer from the saved path\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode (disable dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "def preprocess_text(text, tokenizer, max_length=256):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    # Preprocess the input text\n",
    "    encoding = preprocess_text(text, tokenizer)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits  # Get the raw prediction scores\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1)  # Get the predicted class (0 for negative, 1 for positive)\n",
    "    \n",
    "    # Map the predicted class to sentiment label\n",
    "    sentiment = 'POSITIVE' if predicted_class.item() == 1 else 'NEGATIVE'\n",
    "    \n",
    "    # Return sentiment and probabilities for each class\n",
    "    positive_prob = probabilities[0][1].item()  # Probability for POSITIVE class\n",
    "    negative_prob = probabilities[0][0].item()  # Probability for NEGATIVE class\n",
    "    \n",
    "    return sentiment, positive_prob, negative_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b852a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I absolutely love this movie! It was amazing.\n",
      "Predicted Sentiment: POSITIVE\n",
      "POSITIVE: 0.9998, NEGATIVE: 0.0002\n",
      "\n",
      "Sentence: This movie was terrible, I hated every second of it.\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0005, NEGATIVE: 0.9995\n",
      "\n",
      "Sentence: while this movie is not intended for everyone, it is good for someone has no brain\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.1357, NEGATIVE: 0.8643\n",
      "\n",
      "Sentence: let's watch it only when it is free to watch, i will not pay for it\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0044, NEGATIVE: 0.9956\n",
      "\n",
      "Sentence: A worthy contender for the Animated film of 2024\n",
      "Predicted Sentiment: POSITIVE\n",
      "POSITIVE: 0.9997, NEGATIVE: 0.0003\n",
      "\n",
      "Sentence: No plot at all. But if you are looking for a good laugh. You will not find that either.\n",
      "Predicted Sentiment: NEGATIVE\n",
      "POSITIVE: 0.0021, NEGATIVE: 0.9979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences (positive and negative examples)\n",
    "sample_sentences = [\n",
    "    \"I absolutely love this movie! It was amazing.\",\n",
    "    \"This movie was terrible, I hated every second of it.\", \n",
    "    \"while this movie is not intended for everyone, it is good for someone has no brain\", \n",
    "    \"let's watch it only when it is free to watch, i will not pay for it\",\n",
    "    'A worthy contender for the Animated film of 2024', \n",
    "    'No plot at all. But if you are looking for a good laugh. You will not find that either.'\n",
    "]\n",
    "\n",
    "# Perform inference on each sample sentence\n",
    "for sentence in sample_sentences:\n",
    "    sentiment, positive_prob, negative_prob = predict_sentiment(sentence, model, tokenizer)\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {sentiment}\")\n",
    "    print(f\"POSITIVE: {positive_prob:.4f}, NEGATIVE: {negative_prob:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "dependencies": {
   "environment": {
    "environmentId": "1ae0f57a-c623-40c0-af33-c7d2e8050b04",
    "workspaceId": "b50088d4-10d0-4364-aac0-6ff94657d950"
   },
   "lakehouse": {
    "default_lakehouse": "ec1c56f5-c377-41bf-9ed7-e34e90cf6380",
    "default_lakehouse_name": "Christine",
    "default_lakehouse_workspace_id": "b50088d4-10d0-4364-aac0-6ff94657d950"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
